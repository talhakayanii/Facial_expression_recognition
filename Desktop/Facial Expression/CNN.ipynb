{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5736528d",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, optimizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import ResNet50, Xception\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path \n",
    "import pandas as pd        \n",
    "import keras\n",
    "import numpy as np\n",
    "import os, json\n",
    "from glob import glob\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    cohen_kappa_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ed6fb1",
   "metadata": {},
   "source": [
    "### Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16700fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_ANN=\"../data/annotations\"\n",
    "DATA_PATH_IMG=\"../data/images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149f1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "print(\"Image\")\n",
    "img=plt.imread(f\"{DATA_PATH_IMG}/0.jpg\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed64fdef",
   "metadata": {},
   "source": [
    "#### 0: Neutral, 1: Happy, 2: Sad, 3: Surprise, 4: Fear, 5: Disgust, 6: Anger, 7: Contempt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f178b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expression\n",
    "exp=np.load(f\"{DATA_PATH_ANN}/0_exp.npy\")\n",
    "print(\"Expression: \", exp)\n",
    "print(\"Shape:\", exp.shape)\n",
    "print(\"Data type:\", exp.dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5bd1dc",
   "metadata": {},
   "source": [
    "#### Range [-1,+1] (for Uncertain and Noface categories the value is -2) \n",
    "#### Continuous values from -1 (negative/unpleasant) to +1 (positive/pleasant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d3bcf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valence\n",
    "val=np.load(f\"{DATA_PATH_ANN}/0_val.npy\")\n",
    "print(\"Valence: \", val)\n",
    "print(\"Shape:\", val.shape)\n",
    "print(\"Data type:\", val.dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aee873",
   "metadata": {},
   "source": [
    "#### Range [-1,+1] (for Uncertain and Noface categories the value is -2) \n",
    "#### Continuous values from -1 (Tires) to +1 (Active)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfe8cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arousal\n",
    "aro=np.load(f\"{DATA_PATH_ANN}/0_aro.npy\")\n",
    "print(\"Arousal: \", aro)\n",
    "print(\"Shape:\", aro.shape)\n",
    "print(\"Data type:\", aro.dtype) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6754615b",
   "metadata": {},
   "source": [
    "#### Location of the 68 facial landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360a4731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Landmark\n",
    "lnd=np.load(f\"{DATA_PATH_ANN}/0_lnd.npy\")\n",
    "print(\"Landmark: \", lnd)\n",
    "print(\"Shape:\", lnd.shape)\n",
    "print(\"Data type:\", lnd.dtype) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434c083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "resNet = ResNet50(weights=\"imagenet\")\n",
    "\n",
    "# Print the architecture\n",
    "resNet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd0dc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "xception = Xception(weights=\"imagenet\")\n",
    "\n",
    "xception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9093404a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (128,128)\n",
    "BATCH_SIZE = 24\n",
    "EPOCHS = 35\n",
    "SEED = 42\n",
    "\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d58b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all images\n",
    "img_paths = sorted(glob(os.path.join(DATA_PATH_IMG, \"*.jpg\")))\n",
    "\n",
    "records = []\n",
    "landmarks_list = []\n",
    "\n",
    "for img_path in img_paths:\n",
    "    base = os.path.splitext(os.path.basename(img_path))[0]  # e.g. \"0\"\n",
    "    exp_file = os.path.join(DATA_PATH_ANN, f\"{base}_exp.npy\")\n",
    "    val_file = os.path.join(DATA_PATH_ANN, f\"{base}_val.npy\")\n",
    "    aro_file = os.path.join(DATA_PATH_ANN, f\"{base}_aro.npy\")\n",
    "    lnd_file = os.path.join(DATA_PATH_ANN, f\"{base}_lnd.npy\")\n",
    "\n",
    "    # Load each annotation (each is a tiny .npy file)\n",
    "    expression = int(np.load(exp_file))\n",
    "    valence = float(np.load(val_file))\n",
    "    arousal = float(np.load(aro_file))\n",
    "    landmark = np.load(lnd_file)\n",
    "\n",
    "    # Flatten landmarks if 68x2\n",
    "    if landmark.ndim == 2 and landmark.shape[1] == 2:\n",
    "        landmark = landmark.reshape(-1)\n",
    "\n",
    "    records.append([img_path, expression, valence, arousal])\n",
    "    landmarks_list.append(landmark)\n",
    "\n",
    "# Make DataFrame and landmarks array\n",
    "df = pd.DataFrame(records, columns=[\"image_path\", \"expression\", \"valence\", \"arousal\"])\n",
    "landmarks_array = np.array(landmarks_list, dtype=\"float32\")\n",
    "\n",
    "print(\"Dataset size:\", len(df))\n",
    "print(\"Expression classes:\", np.unique(df[\"expression\"]))\n",
    "print(\"Landmarks shape:\", landmarks_array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b858a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc51e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Split into train/val ---\n",
    "train_df, val_df, train_landmarks, val_landmarks = train_test_split(\n",
    "    df, landmarks_array, test_size=0.2, stratify=df[\"expression\"], random_state=SEED\n",
    ")\n",
    "\n",
    "# --- Helper to load images ---\n",
    "def _load_image(path, img_size=IMG_SIZE):\n",
    "    img = tf.io.read_file(path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)   # force RGB\n",
    "    img = tf.image.resize(img, img_size)\n",
    "    img = tf.cast(img, tf.float32) / 255.0\n",
    "    return img\n",
    "\n",
    "# --- Dataset builder ---\n",
    "def make_multimodal_dataset(df_subset, lm_array, batch_size=BATCH_SIZE, shuffle=False):\n",
    "    def gen():\n",
    "        for p, exp, val, aro, lm in zip(\n",
    "            df_subset[\"image_path\"], df_subset[\"expression\"],\n",
    "            df_subset[\"valence\"], df_subset[\"arousal\"], lm_array\n",
    "        ):\n",
    "            yield p.encode(\"utf-8\"), lm.astype(\"float32\"), exp, np.array([val, aro], dtype=\"float32\")\n",
    "    \n",
    "    output_types = (tf.string, tf.float32, tf.int64, tf.float32)\n",
    "    output_shapes = ((), (lm_array.shape[1],), (), (2,))\n",
    "    ds = tf.data.Dataset.from_generator(gen, output_types=output_types, output_shapes=output_shapes)\n",
    "    \n",
    "    def _map(path, lm, exp, va):\n",
    "        img = _load_image(path)\n",
    "        return {\"image_input\": img, \"lm_input\": lm}, {\"expr_out\": tf.one_hot(exp, df[\"expression\"].nunique()), \"va_out\": va}\n",
    "    \n",
    "    ds = ds.map(_map, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(1024, seed=SEED)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# --- Create train/val datasets ---\n",
    "train_ds = make_multimodal_dataset(train_df, train_landmarks, shuffle=True)\n",
    "val_ds   = make_multimodal_dataset(val_df, val_landmarks, shuffle=False)\n",
    "\n",
    "print(train_ds)\n",
    "print(val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4317ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (128, 128, 3)\n",
    "LM_DIM = train_landmarks.shape[1]\n",
    "NUM_CLASSES = df[\"expression\"].nunique()\n",
    "\n",
    "def build_resnet_with_landmarks(img_shape=IMG_SHAPE, lm_dim=LM_DIM, freeze_backbone=True):\n",
    "    # --- Image branch (ResNet50) ---\n",
    "    img_in = layers.Input(shape=img_shape, name=\"image_input\")\n",
    "    base = ResNet50(weights=\"imagenet\", include_top=False, input_tensor=img_in)\n",
    "    if freeze_backbone:\n",
    "        base.trainable = False\n",
    "    x = base.output\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "    x = layers.Dense(512, activation=\"relu\")(x)\n",
    "    x = layers.Dropout(0.4)(x)\n",
    "    img_feat = layers.Dense(256, activation=\"relu\")(x)\n",
    "\n",
    "    # --- Landmark branch ---\n",
    "    lm_in = layers.Input(shape=(lm_dim,), name=\"lm_input\")\n",
    "    y = layers.Dense(128, activation=\"relu\")(lm_in)\n",
    "    y = layers.Dropout(0.3)(y)\n",
    "    lm_feat = layers.Dense(64, activation=\"relu\")(y)\n",
    "\n",
    "    # --- Fusion ---\n",
    "    fused = layers.Concatenate()([img_feat, lm_feat])\n",
    "    fused = layers.Dense(256, activation=\"relu\")(fused)\n",
    "    fused = layers.Dropout(0.4)(fused)\n",
    "\n",
    "    # --- Heads ---\n",
    "    expr_out = layers.Dense(NUM_CLASSES, activation=\"softmax\", name=\"expr_out\")(fused)\n",
    "    va_out = layers.Dense(2, activation=\"tanh\", name=\"va_out\")(fused)\n",
    "\n",
    "    model = models.Model(inputs=[img_in, lm_in], outputs=[expr_out, va_out])\n",
    "    return model\n",
    "\n",
    "# Build model with frozen backbone first\n",
    "model = build_resnet_with_landmarks(freeze_backbone=True)\n",
    "\n",
    "# Unfreeze last ~30 layers (fine-tuning)\n",
    "for layer in model.layers[-100:]:\n",
    "    if not isinstance(layer, layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "\n",
    "# Re-compile with smaller learning rate for fine-tuning\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-5),   # <-- smaller LR\n",
    "    loss={\"expr_out\": \"categorical_crossentropy\", \"va_out\": \"mse\"},\n",
    "    loss_weights={\"expr_out\": 1.0, \"va_out\": 1.0},\n",
    "    metrics={\"expr_out\": \"accuracy\"}\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a2b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Callbacks ---\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        \"best_resnet_landmarks.h5\", monitor=\"val_expr_out_accuracy\",\n",
    "        save_best_only=True, mode=\"max\", verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor=\"val_expr_out_accuracy\", factor=0.5, patience=3,\n",
    "        mode=\"max\", verbose=1\n",
    "    ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_expr_out_accuracy\", patience=7,\n",
    "        mode=\"max\", restore_best_weights=True, verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# --- Train ---\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dc15e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase 1: Build frozen model\n",
    "model = build_resnet_with_landmarks(freeze_backbone=True)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-4),\n",
    "    loss={\"expr_out\": \"categorical_crossentropy\", \"va_out\": \"mse\"},\n",
    "    loss_weights={\"expr_out\": 1.0, \"va_out\": 1.0},\n",
    "    metrics={\"expr_out\": \"accuracy\"}\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"warmup_resnet.h5\", monitor=\"val_expr_out_accuracy\",\n",
    "                                       save_best_only=True, mode=\"max\", verbose=1)\n",
    "]\n",
    "\n",
    "history_warmup = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c41003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear old graph\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "# Reload best warmup weights into a fresh model\n",
    "model = build_resnet_with_landmarks(freeze_backbone=True)\n",
    "model.load_weights(\"warmup_resnet.h5\")\n",
    "\n",
    "# Re-compile with smaller LR\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=1e-5),\n",
    "    loss={\"expr_out\": \"categorical_crossentropy\", \"va_out\": \"mse\"},\n",
    "    loss_weights={\"expr_out\": 1.0, \"va_out\": 1.0},\n",
    "    metrics={\"expr_out\": \"accuracy\"}\n",
    ")\n",
    "\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\"finetuned_resnet.h5\", monitor=\"val_expr_out_accuracy\",\n",
    "                                       save_best_only=True, mode=\"max\", verbose=1),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_expr_out_accuracy\", factor=0.5, patience=3, mode=\"max\"),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_expr_out_accuracy\", patience=7, mode=\"max\", restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history_finetune = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=25,\n",
    "    callbacks=callbacks\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142bb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 1. Imports\n",
    "# ==========================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, f1_score, cohen_kappa_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# plotting (optional for confusion matrix / report)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# if needed\n",
    "from PIL import Image\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from torchvision import models, transforms\n",
    "\n",
    "# optional timm\n",
    "try:\n",
    "    import timm\n",
    "\n",
    "    TIMM_AVAILABLE = True\n",
    "except Exception:\n",
    "    TIMM_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7e583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 2. Config\n",
    "# ==========================\n",
    "DATA_PATH_IMG = \"../data/images\"        # <-- change if needed\n",
    "DATA_PATH_ANN = \"../data/annotations\"   # <-- change if needed\n",
    "IMG_SIZE = 128\n",
    "BATCH_SIZE = 24\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba555fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    IMG_SIZE = 128\n",
    "    BATCH_SIZE = 24\n",
    "    NUM_WORKERS = 4\n",
    "    LR = 1e-4\n",
    "    EPOCHS = 25\n",
    "    SEED = 42\n",
    "    SAVE_DIR = Path(\"outputs\")\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    PRINT_FREQ = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9194a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 3. Dataset\n",
    "# ==========================\n",
    "class AffectDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.transform = transform\n",
    "\n",
    "        self.img_paths = sorted(glob(os.path.join(img_dir, \"*.jpg\")))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        base = os.path.splitext(os.path.basename(img_path))[0]\n",
    "\n",
    "        exp_file = os.path.join(self.ann_dir, f\"{base}_exp.npy\")\n",
    "        val_file = os.path.join(self.ann_dir, f\"{base}_val.npy\")\n",
    "        aro_file = os.path.join(self.ann_dir, f\"{base}_aro.npy\")\n",
    "        lnd_file = os.path.join(self.ann_dir, f\"{base}_lnd.npy\")\n",
    "\n",
    "        # Load\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        expression = int(np.load(exp_file))\n",
    "        valence = float(np.load(val_file))\n",
    "        arousal = float(np.load(aro_file))\n",
    "        landmark = np.load(lnd_file)\n",
    "\n",
    "        if landmark.ndim == 2:  # flatten 68x2 -> 136\n",
    "            landmark = landmark.reshape(-1)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"landmarks\": torch.tensor(landmark, dtype=torch.float32),\n",
    "            \"expression\": torch.tensor(expression, dtype=torch.long),\n",
    "            \"valence\": torch.tensor(valence, dtype=torch.float32),\n",
    "            \"arousal\": torch.tensor(arousal, dtype=torch.float32),\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        \"image\": torch.stack([b[\"image\"] for b in batch]),\n",
    "        \"landmarks\": torch.stack([b[\"landmarks\"] for b in batch]),\n",
    "        \"expression\": torch.stack([b[\"expression\"] for b in batch]),\n",
    "        \"valence\": torch.stack([b[\"valence\"] for b in batch]),\n",
    "        \"arousal\": torch.stack([b[\"arousal\"] for b in batch]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9ff3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 4. Model\n",
    "# ==========================\n",
    "class MultiTaskModel(nn.Module):\n",
    "    def __init__(self, num_classes=8, lm_dim=136, backbone_name=\"resnet50\", pretrained=False):\n",
    "        super().__init__()\n",
    "        if backbone_name == \"resnet50\":\n",
    "            base = models.resnet50(weights=\"IMAGENET1K_V1\" if pretrained else None)\n",
    "        else:\n",
    "            raise ValueError(\"Only resnet50 implemented here\")\n",
    "\n",
    "        # remove FC\n",
    "        modules = list(base.children())[:-1]\n",
    "        self.cnn = nn.Sequential(*modules)\n",
    "        feat_dim = base.fc.in_features\n",
    "\n",
    "        # CNN branch\n",
    "        self.cnn_fc = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Landmarks branch\n",
    "        self.lm_fc = nn.Sequential(\n",
    "            nn.Linear(lm_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "\n",
    "        # Fusion\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Linear(256 + 128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4)\n",
    "        )\n",
    "\n",
    "        # Heads\n",
    "        self.expr_head = nn.Linear(256, num_classes)\n",
    "        self.va_head = nn.Linear(256, 2)\n",
    "\n",
    "    def forward(self, x_img, x_lm):\n",
    "        feat_img = self.cnn(x_img).view(x_img.size(0), -1)\n",
    "        feat_img = self.cnn_fc(feat_img)\n",
    "\n",
    "        feat_lm = self.lm_fc(x_lm)\n",
    "\n",
    "        fused = self.fusion(torch.cat([feat_img, feat_lm], dim=1))\n",
    "\n",
    "        expr_logits = self.expr_head(fused)\n",
    "        va_out = self.va_head(fused)\n",
    "\n",
    "        return expr_logits, va_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0972f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 5. Metrics (classification + regression)\n",
    "# ==========================\n",
    "def ccc(a, b):\n",
    "    a_mean, b_mean = a.mean(), b.mean()\n",
    "    cov = ((a - a_mean) * (b - b_mean)).mean()\n",
    "    var_a, var_b = a.var(), b.var()\n",
    "    return (2 * cov) / (var_a + var_b + (a_mean - b_mean) ** 2 + 1e-8)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    y_true_exp, y_pred_exp = [], []\n",
    "    y_true_va, y_pred_va = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            imgs = batch[\"image\"].to(device)\n",
    "            lms = batch[\"landmarks\"].to(device)\n",
    "            expr = batch[\"expression\"].to(device)\n",
    "            val = batch[\"valence\"].cpu().numpy()\n",
    "            aro = batch[\"arousal\"].cpu().numpy()\n",
    "\n",
    "            expr_logits, va_out = model(imgs, lms)\n",
    "            pred_expr = torch.argmax(expr_logits, dim=1)\n",
    "\n",
    "            y_true_exp.extend(expr.cpu().numpy())\n",
    "            y_pred_exp.extend(pred_expr.cpu().numpy())\n",
    "\n",
    "            y_true_va.extend(np.vstack([val, aro]).T)\n",
    "            y_pred_va.extend(va_out.cpu().numpy())\n",
    "\n",
    "    y_true_exp = np.array(y_true_exp)\n",
    "    y_pred_exp = np.array(y_pred_exp)\n",
    "    y_true_va = np.array(y_true_va)\n",
    "    y_pred_va = np.array(y_pred_va)\n",
    "\n",
    "    # classification metrics\n",
    "    acc = accuracy_score(y_true_exp, y_pred_exp)\n",
    "    f1 = f1_score(y_true_exp, y_pred_exp, average=\"macro\")\n",
    "    kappa = cohen_kappa_score(y_true_exp, y_pred_exp)\n",
    "\n",
    "    # regression metrics\n",
    "    rmse = np.sqrt(np.mean((y_true_va - y_pred_va) ** 2, axis=0))\n",
    "    pear = [pearsonr(y_true_va[:, i], y_pred_va[:, i])[0] for i in range(2)]\n",
    "    cccs = [ccc(y_true_va[:, i], y_pred_va[:, i]) for i in range(2)]\n",
    "\n",
    "    return {\n",
    "        \"acc\": acc,\n",
    "        \"f1\": f1,\n",
    "        \"kappa\": kappa,\n",
    "        \"rmse_val\": rmse[0],\n",
    "        \"rmse_aro\": rmse[1],\n",
    "        \"pear_val\": pear[0],\n",
    "        \"pear_aro\": pear[1],\n",
    "        \"ccc_val\": cccs[0],\n",
    "        \"ccc_aro\": cccs[1],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d284364",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================\n",
    "# 6. Run evaluation\n",
    "# ==========================\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "ds = AffectDataset(DATA_PATH_IMG, DATA_PATH_ANN, transform=transform)\n",
    "n = len(ds)\n",
    "test_size = int(0.15 * n)\n",
    "val_size = test_size\n",
    "train_size = n - val_size - test_size\n",
    "_, _, test_ds = random_split(ds, [train_size, val_size, test_size],\n",
    "                             generator=torch.Generator().manual_seed(SEED))\n",
    "\n",
    "test_loader = DataLoader(test_ds, batch_size=BATCH_SIZE,\n",
    "                         shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "# build model\n",
    "model = MultiTaskModel(num_classes=len(np.unique([d['expression'] for d in ds])),\n",
    "                       lm_dim=ds[0]['landmarks'].numel(),\n",
    "                       pretrained=False).to(DEVICE)\n",
    "\n",
    "# load weights# Allow numpy scalar types for this session\n",
    "torch.serialization.add_safe_globals([np._core.multiarray.scalar])\n",
    "\n",
    "# Now load safely\n",
    "checkpoint = torch.load(\"../output/best_resnet18.pth\", map_location=CFG.DEVICE, weights_only=False)\n",
    "\n",
    "# Access model weights\n",
    "model.load_state_dict(checkpoint[\"model_state\"])\n",
    "model.eval()\n",
    "\n",
    "# evaluate\n",
    "metrics = evaluate(model, test_loader, DEVICE)\n",
    "print(\"\\n📊 Evaluation Results:\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"{k:12s}: {v:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
